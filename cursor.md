# Cursor Guide: Hugging Face Transformers SEO Starter Project

This guide helps AI assistants understand the context, structure, and best practices for this learning project focused on teaching Hugging Face Transformers through practical SEO/marketing tasks.

## Project Overview

This is an educational starter project that teaches modern NLP using Hugging Face Transformers in a practical, marketer-focused context. The project covers 6 sequential tasks that build from basic classification to advanced semantic search, all applied to real-world SEO and marketing scenarios.

**Learning Path:**
1. Intent Classification (Zero-shot) → 2. Keyword Clustering → 3. Title/Meta Generation → 4. NER → 5. Summarization → 6. Semantic Search

**Target Audience:** Marketers, technical SEO professionals, and beginners learning transformers through practical applications.

**Setup:** See `README.md` for installation instructions. Requires Python 3.8+, virtual environment, and dependencies from `requirements.txt`.

## Project Structure

### Dual Structure: Learning vs Production

- **`notebooks/`** - Educational Jupyter notebooks for learning each concept
  - `01_intent_classification.ipynb` - Zero-shot classification introduction
  - `02_keyword_clustering.ipynb` - Embeddings and clustering basics
  - `03_title_meta_generation.ipynb` - Text generation for SEO
  - `04_ner_entities.ipynb` - Named entity recognition
  - `05_summarization.ipynb` - Content summarization
  - `06_semantic_search.ipynb` - Semantic search with embeddings

- **`src/`** - Production-ready Python scripts with CLI arguments
  - `intent_classifier.py` - Intent classification script
  - `cluster_keywords.py` - Keyword clustering script
  - `generate_meta.py` - Title/meta generation script
  - `extract_entities.py` - NER extraction script
  - `summarize.py` - Summarization script
  - `semantic_search.py` - Semantic search script
  - `utils.py` - Common utility functions (CSV reading, saving, validation)

### Data Structure

- **`data/examples/`** - Sample CSV files for testing
  - `keywords.csv` - Input format: `keyword,volume,niche` columns (required: `keyword`)
  - `serp_samples.csv` - SERP data: `query,title,snippet,url,est_traffic,conversion_rate`
  - Output files are generated by scripts (e.g., `keywords_intent.csv`, `keywords_clusters.csv`)

## Task-Specific Context

### 01: Intent Classification (Zero-Shot)

**Purpose:** Classify search keywords into 4 intent categories to inform content strategy and keyword grouping.

**Model:** `facebook/bart-large-mnli` (BART-large with MultiNLI for zero-shot classification)

**Labels:**
- `informational` - User wants to learn/understand something
- `navigational` - User wants to find a specific website/brand
- `commercial` - User is researching products/services (pre-purchase)
- `transactional` - User wants to buy/perform an action

**Implementation:**
- Uses `pipeline('zero-shot-classification', model=...)` from transformers
- Processes keywords from CSV one at a time (can be batched for performance)
- Outputs: `keyword`, `intent`, `confidence` score
- Script: `src/intent_classifier.py` accepts `--input`, `--output`, `--model` args

**SEO Application:** 
- Group keywords by intent for content planning
- Identify content gaps (e.g., missing informational content)
- Optimize landing pages for transactional vs informational queries

**Model Alternatives:**
- Faster (less accurate): Smaller NLI models like `roberta-large-mnli` or `typeform/distilbert-base-uncased-mnli`
- More accurate: `facebook/bart-large-mnli` (default, slower on CPU)

### 02: Keyword Clustering

**Purpose:** Group semantically similar keywords into topic clusters using embeddings and KMeans clustering.

**Model:** `sentence-transformers/all-MiniLM-L6-v2` (fast, high-quality embeddings for size)

**Implementation:**
- Encodes keywords into embeddings using SentenceTransformer
- Normalizes embeddings (important for cosine similarity)
- Applies KMeans clustering (default: 5 clusters, configurable via `--clusters`)
- Finds representative keywords (closest to cluster centroids)
- Outputs: `keyword`, `cluster`, `cluster_rep` columns
- Script: `src/cluster_keywords.py` with `--input`, `--output`, `--model`, `--clusters` args

**SEO Application:**
- Topic modeling for content pillar strategy
- Identify content gaps by analyzing cluster sizes
- Group related keywords for internal linking opportunities
- Content ideation based on cluster themes

**Model Alternatives:**
- Larger (better quality): `sentence-transformers/all-mpnet-base-v2`
- Faster: `sentence-transformers/all-MiniLM-L6-v2` (default, good balance)

### 03: Title/Meta Generation

**Purpose:** Generate SEO-optimized page titles and meta descriptions for keyword clusters.

**Model:** `google/flan-t5-small` (T5-based text-to-text generation model)

**Implementation:**
- Uses `pipeline('text2text-generation', model=...)`
- Prompt template: Generates both title (~60 chars) and meta (150-160 chars) in one pass
- Parses output from format: `Title: <title> || Meta: <meta>`
- Falls back to simple parsing if format doesn't match
- Input: CSV with `cluster_rep` column (from clustering step)
- Output: `cluster_rep`, `title`, `meta_description`
- Script: `src/generate_meta.py` with `--input`, `--output`, `--model`, `--max_new_tokens` args

**SEO Application:**
- Bulk generation of titles/meta for content calendars
- A/B testing different title variations
- Maintaining consistent tone and brand voice
- Ensuring keyword inclusion in titles and descriptions

**Best Practices:**
- Titles should be ~60 characters (Google display limit)
- Meta descriptions: 150-160 characters for optimal SERP display
- Include primary keyword naturally
- Maintain authoritative but friendly tone

**Model Alternatives:**
- Better quality: `google/flan-t5-base` or `google/flan-t5-large` (slower)
- Faster: `google/flan-t5-small` (default, good for bulk generation)

### 04: Named Entity Recognition (NER)

**Purpose:** Extract entities (organizations, locations, persons, miscellaneous) from SERP snippets to inform content briefs and structured data.

**Model:** `dslim/bert-base-NER` (BERT fine-tuned on CoNLL-2003, recognizes ORG, LOC, PER, MISC)

**Implementation:**
- Uses `pipeline('ner', model=..., aggregation_strategy='simple')`
- Processes combined `title + snippet` text from SERP data
- Aggregation strategy groups subword tokens into complete entities
- Outputs: `query`, `entities` (semicolon-separated format: `Entity(ENTITY_GROUP)`)
- Script: `src/extract_entities.py` with `--input`, `--output`, `--model` args

**Entity Types:**
- `ORG` - Organizations (brands, companies)
- `LOC` - Locations (cities, countries, places)
- `PER` - Persons (names of people)
- `MISC` - Miscellaneous entities

**SEO Application:**
- Competitive analysis (identify brands mentioned in SERPs)
- Location-based content optimization
- Structured data markup (Schema.org Person, Organization, Place)
- Brand monitoring across search results
- Content brief creation (identify key entities to mention)

**Model Alternatives:**
- More accurate: `dbmdz/bert-large-cased-finetuned-conll03-english`
- Multi-lingual: `xlm-roberta-large-finetuned-conll03-english` (if needed)

### 05: Summarization

**Purpose:** Condense long content (competitor pages, research notes) into brief summaries for faster strategy development.

**Model:** `sshleifer/distilbart-cnn-12-6` (DistilBART fine-tuned on CNN/DailyMail, extractive summarization)

**Implementation:**
- Uses `pipeline('summarization', model=...)`
- Truncates input to 3000 characters if longer (model context limit)
- Generates summaries with `max_new_tokens=128` (configurable)
- Script: `src/summarize.py` with `--input_text`, `--model` args
- Can be extended to process CSV files batch-wise

**SEO Application:**
- Content audits (summarize competitor pages quickly)
- SERP analysis (summarize top-ranking pages for a query)
- Research note condensation (faster strategy development)
- Content gap analysis (quick understanding of existing content)

**Model Alternatives:**
- Better quality: `facebook/bart-large-cnn` (slower, larger)
- Faster: `sshleifer/distilbart-cnn-12-6` (default, good balance)
- Abstractive: `google/pegasus-xsum` (different style, more creative)

### 06: Semantic Search

**Purpose:** Build a semantic search index over SERP results or research notes to find content by meaning (not just keyword matching).

**Model:** `sentence-transformers/all-MiniLM-L6-v2` (same as clustering, for embeddings)

**Implementation:**
- Uses SentenceTransformer to encode corpus texts into embeddings
- Builds a NearestNeighbors index (cosine similarity, sklearn)
- Normalizes embeddings for accurate cosine distance
- Queries return top-k most similar texts with distance scores
- Input: CSV with `title` and `snippet` columns (SERP data)
- Script: `src/semantic_search.py` with `--csv`, `--query` args

**SEO Application:**
- Content ideation (find similar content to a topic)
- Keyword expansion (semantic variations of a seed keyword)
- Content matching (find best pages for a query)
- Research discovery (find related SERP results)

**Performance Notes:**
- For large corpora (1000+ documents), consider FAISS for faster indexing
- Current implementation uses sklearn's NearestNeighbors (good for <10k docs)
- Cosine similarity is the standard metric for semantic search

## Common Patterns & Best Practices

### Model Selection Guidance

**CPU-Friendly Models (Recommended for Learning):**
- Zero-shot: `facebook/bart-large-mnli` (accurate but slower), or `roberta-large-mnli` (faster)
- Embeddings: `sentence-transformers/all-MiniLM-L6-v2` (fast, good quality)
- Generation: `google/flan-t5-small` (fast, decent quality)
- Summarization: `sshleifer/distilbart-cnn-12-6` (balanced)
- NER: `dslim/bert-base-NER` (standard)

**Speed vs Accuracy Trade-offs:**
- For prototyping: Use smaller models (faster iteration)
- For production: Test larger models, measure accuracy vs latency
- Batch processing: Always batch when possible (encode multiple texts at once)

### CSV Handling Patterns

**Input Validation:**
- Use `src/utils.py` functions: `read_keywords_csv()`, `read_serp_csv()`
- `read_keywords_csv()` validates that `keyword` column exists
- `ensure_cols()` helper validates required columns exist

**Data Flow:**
1. Start with `keywords.csv` or `serp_samples.csv`
2. Scripts generate intermediate/output CSVs
3. Some scripts chain: clustering → generate_meta (requires `cluster_rep` column)

**CSV Structure:**
- `keywords.csv`: `keyword,volume,niche` (keyword required)
- `serp_samples.csv`: `query,title,snippet,url,est_traffic,conversion_rate`
- Output files add columns like `intent`, `cluster`, `cluster_rep`, `entities`

### Error Handling

**Common Issues:**
- Missing columns: Use `ensure_cols()` or check CSV structure before processing
- Model download failures: Check internet connection, HF token if needed (see `.env`)
- CUDA/GPU issues: Models default to CPU; ensure `torch` is CPU version if no GPU
- Memory issues: Process in batches, use smaller models, or reduce batch size

**Model Loading:**
- First run downloads models from Hugging Face Hub (can be slow)
- Models cached in `~/.cache/huggingface/` after first download
- Use `.env` file with `HF_TOKEN` if accessing gated models

### Performance Tips

**Batch Processing:**
- Embeddings: `model.encode(list_of_texts, ...)` processes multiple texts efficiently
- Classification: Can batch zero-shot classification (pass list of texts)
- Always use `show_progress_bar=True` for user feedback on long operations

**Caching:**
- Models are cached after first load (no need to reload between script runs)
- Consider caching embeddings if running multiple analyses on same data

**Progress Bars:**
- Use `tqdm` or built-in progress bars (`show_progress_bar=True` in SentenceTransformer)
- Helps users understand processing time for large datasets

## SEO/Marketing Context

### Business Value of Each Task

**Intent Classification:**
- Content strategy: Align content type with search intent
- Landing page optimization: Match page type to intent (blog vs product page)
- Keyword grouping: Organize keyword lists by intent for content planning

**Keyword Clustering:**
- Topic modeling: Identify content pillar opportunities
- Content gaps: Analyze cluster sizes to find underrepresented topics
- Internal linking: Group related content for cross-linking opportunities

**Title/Meta Generation:**
- Bulk content production: Generate titles/metas for content calendars
- A/B testing: Generate variations for testing
- Consistency: Maintain brand voice across all pages
- Efficiency: Speed up manual title/meta writing

**NER (Entity Extraction):**
- Competitive analysis: Identify which brands appear in SERPs
- Structured data: Enrich Schema.org markup with entities
- Brand monitoring: Track brand mentions across search results
- Content briefs: Identify key entities to include in content

**Summarization:**
- Content audits: Quickly understand competitor content
- SERP analysis: Summarize top-ranking pages for insights
- Research efficiency: Condense long articles into actionable notes

**Semantic Search:**
- Content ideation: Find similar content to expand on
- Keyword expansion: Discover semantic variations of seed keywords
- Content matching: Find best existing pages for new queries
- Research discovery: Find related content across large document sets

### SEO Best Practices in Context

**Title Generation:**
- Keep titles ~60 characters (Google truncates at ~60)
- Include primary keyword near the start
- Maintain natural, readable language
- Avoid keyword stuffing

**Meta Descriptions:**
- Aim for 150-160 characters (optimal SERP display)
- Include call-to-action when appropriate
- Naturally incorporate primary and secondary keywords
- Match search intent (informational vs transactional tone)

**Entity Extraction:**
- Use extracted entities for Schema.org structured data
- Identify brand opportunities (mention competitors if relevant)
- Location-based content optimization (local SEO)

## Troubleshooting Guide

### Common Errors

**Model Download Failures:**
- **Error:** `ConnectionError` or timeout downloading models
- **Solution:** Check internet connection, retry. For gated models, add `HF_TOKEN` to `.env` file
- **Workaround:** Download models manually or use offline mode with pre-downloaded models

**CUDA/GPU Issues:**
- **Error:** `CUDA out of memory` or CUDA not available
- **Solution:** Ensure CPU-only PyTorch if no GPU: `pip install torch --index-url https://download.pytorch.org/whl/cpu`
- **Note:** All recommended models work on CPU (may be slower)

**CSV Format Problems:**
- **Error:** `ValueError: CSV must include a 'keyword' column`
- **Solution:** Ensure CSV has required columns. Check `src/utils.py` for expected format
- **Common:** Missing headers, wrong column names, or empty CSV files

**Memory Issues:**
- **Error:** `MemoryError` or system slowdown
- **Solution:** 
  - Process in smaller batches
  - Use smaller models (e.g., `all-MiniLM-L6-v2` instead of `all-mpnet-base-v2`)
  - Reduce `--clusters` parameter in clustering
  - Process fewer rows at a time

**Model Loading Slow:**
- **Issue:** First-time model loading takes a long time
- **Explanation:** Models download from Hugging Face Hub on first use
- **Solution:** Normal behavior; subsequent runs use cached models (much faster)

### Model Alternatives

**When Primary Model Fails or Too Slow:**

**Intent Classification:**
- Faster: `typeform/distilbert-base-uncased-mnli` (less accurate)
- More accurate: `facebook/bart-large-mnli` (default, slower)

**Embeddings (Clustering & Search):**
- Larger/better: `sentence-transformers/all-mpnet-base-v2` (slower, higher quality)
- Faster: `sentence-transformers/all-MiniLM-L6-v2` (default, good balance)

**Generation:**
- Better quality: `google/flan-t5-base` or `google/flan-t5-large` (slower)
- Faster: `google/flan-t5-small` (default)

**Summarization:**
- Better quality: `facebook/bart-large-cnn` (slower, larger)
- Faster: `sshleifer/distilbart-cnn-12-6` (default)

**NER:**
- More accurate: `dbmdz/bert-large-cased-finetuned-conll03-english` (larger)
- Multi-lingual: `xlm-roberta-large-finetuned-conll03-english` (if needed)

### Resource Constraints

**CPU-Only Setup:**
- All recommended models work on CPU
- Expect slower processing (especially for larger models)
- Use smaller models for faster iteration
- Batch processing still helps significantly

**Memory Limits:**
- Embeddings: Process in batches (e.g., 100-1000 texts at a time)
- Generation: Generate one at a time if memory constrained
- Clustering: Reduce `--clusters` parameter for smaller datasets

**Large Datasets:**
- For 10k+ keywords: Process in batches, use smaller models
- Consider FAISS for semantic search on large corpora (instead of sklearn)
- Use progress bars to monitor long-running operations

## Code Patterns

### Standard Script Structure

All scripts in `src/` follow a consistent pattern:

```python
import argparse
from transformers import pipeline  # or SentenceTransformer, etc.
from src.utils import read_keywords_csv, save_csv  # or read_serp_csv

def main_function(input_csv: str, output_csv: str, model_name: str = "default-model"):
    # 1. Load data using utils
    df = read_keywords_csv(input_csv)  # or read_serp_csv
    
    # 2. Load model
    model = pipeline(...)  # or SentenceTransformer(...)
    
    # 3. Process data
    results = []
    for item in df['column']:
        result = model(item)
        results.append(result)
    
    # 4. Save using utils
    out_df = pd.DataFrame(results)
    save_csv(out_df, output_csv)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", default="data/examples/input.csv")
    parser.add_argument("--output", default="data/examples/output.csv")
    parser.add_argument("--model", default="default-model")
    args = parser.parse_args()
    main_function(args.input, args.output, args.model)
```

### Utility Functions (`src/utils.py`)

**Key Functions:**
- `read_keywords_csv(path)` - Reads CSV, validates `keyword` column exists
- `read_serp_csv(path)` - Reads SERP CSV files
- `ensure_cols(df, cols)` - Validates required columns exist
- `save_csv(df, path)` - Saves DataFrame to CSV, prints confirmation
- `load_env()` - Loads `.env` file (for HF_TOKEN, etc.)

### Environment Variables

**`.env` file (optional):**
- `HF_TOKEN` - Hugging Face token for accessing gated models (if needed)
- Load with `from src.utils import load_env; load_env()` at script start

### Chaining Scripts

**Common Workflow:**
1. `intent_classifier.py` → adds `intent` column
2. `cluster_keywords.py` → adds `cluster` and `cluster_rep` columns
3. `generate_meta.py` → requires `cluster_rep`, generates `title` and `meta_description`

**Example Chain:**
```bash
python src/intent_classifier.py --input data/examples/keywords.csv --output data/examples/keywords_intent.csv
python src/cluster_keywords.py --input data/examples/keywords_intent.csv --output data/examples/keywords_clusters.csv
python src/generate_meta.py --input data/examples/keywords_clusters.csv --output data/examples/seo_titles_meta.csv
```

## Additional Notes

- **Learning First:** Students should explore `notebooks/` before using `src/` scripts
- **Model Downloads:** First run of each script downloads models (slow); subsequent runs are fast
- **Output Files:** Scripts generate CSV files in `data/examples/` by default (configurable via `--output`)
- **Extensibility:** Scripts are designed to be extended (add more models, features, output formats)
- **Production Use:** Scripts can be integrated into larger SEO tools or workflows

